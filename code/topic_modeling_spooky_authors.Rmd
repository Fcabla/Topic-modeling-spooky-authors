---
title: "Topic modelling on ds spooky authors"
output: html_notebook
---

## Description of the problem

The problem to be solved in this natural language processing project is to discover possible abstract themes that occur in a collection of documents. Topic modeling is a very useful tool when trying to discover hidden semantic structures in texts.
The dataset to be used contains excerpts from horror novels by authors Edgar Allan Poe (EAP), Mary Shelley (MWS), and HP Lovecraft (HPL). This dataset comes from a competition in 2017 on the popular Kaggle web community, it can be found at the following web URL:
https://www.kaggle.com/c/spooky-author-identification

In this competition, participants were asked to build a model that was capable of predicting the author of a specific document, also to share new knowledge about the data.



## Load libraries

```{r}
library(tm)
library(ggplot2)
library(reshape2)
library(wordcloud)
library(RWeka)

library(tidytext) # tidy implimentation of NLP methods
library(tidyverse) # general utility & workflow functions

# Lemmatization
library(textstem)

# LDA (topic modelling)
library(topicmodels)

```

## Load Files
```{r}
# Reading the data
df <- read.csv("data/train.csv")
head(df)
```


## Corpus

Transforming the data frame in to corpus format.
```{r}
# Rename id column to "doc_id"
names(df)[1] <- "doc_id"

# Convert to corpus
corpus <- Corpus(DataframeSource(df))
```

Check the length of the corpus.
```{r}
length(corpus)
```

Inspecting the first entry.
```{r}
inspect(corpus[[1]])
```

Checking the metadata of the first entry.
```{r}
meta(corpus[[1]])
```


## Text cleaning

Print some instances to check the data.
```{r}
inspect(corpus[1:5])
```
Check the possible transformations on the corpus.
```{r}
getTransformations()
```
Remove punctuation signs of the corpus.
```{r}
corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])
```
Set all the characters to lower case.
```{r}
corpus <- tm_map(corpus, content_transformer(tolower))
inspect(corpus[1:5])
```

Stemming   
Converts all variants of a term to the same term, an example: the terms: running, runner, ran are converted to run after stemming the document. 
Stemming algorithms work by cutting off the end or the beginning of the word.
```{r}
corpus_temp <- tm_map(corpus, stemDocument)
inspect(corpus_temp[1:5])
```
We are not going to apply stemming, instead we are going to use lemmatization.

The main difference is that lemmatization considers the morphological analysis of the words.

```{r}
corpus <- tm_map(corpus, lemmatize_strings)
inspect(corpus[1:5])
```


Remove numbers even though in the firsts instances we did not found any number. 
```{r}
corpus <- tm_map(corpus, removeNumbers)
```

Remove the extra whitespaces.
```{r}
corpus <- tm_map(corpus, stripWhitespace)
```


## Stop Words removal

Stop words are those words which appear the most in any language. They usually are words without any meaning by themselves, lets check the stopwords.

```{r}
stopwords()
```
Remove stopwords
```{r}
inspect(corpus[[1]])
corpus = tm_map(corpus,removeWords,stopwords())
inspect(corpus[[1]])
```

## Common words

Apart from the stopwords of the tdm package we can also identify the terms that are repeated the most and delete them from the corpus since they probably do not provide any new knowledge. 
```{r}
tdm = TermDocumentMatrix(corpus)
tdm
```

After all the transformations, lets check how many terms have been identified in the TDM
```{r}
length(dimnames(tdm)$Terms)
```
Which terms are the most frequent and which are the least.
```{r}
fr=rowSums(as.matrix(tdm))
common_words <- tail(sort(fr),20)
common_words

# delete some terms from the most common words
common_words <- common_words[names(common_words) %in% c("man", "time", "day", "eye") == FALSE]
```
Check which terms only appears once
```{r}
sum(fr == 1)
```
6584 of the 18007 terms only appears once.

Remove the new stop words (common words)
```{r}
new_stop_words <- names(common_words)
corpus = tm_map(corpus,removeWords,new_stop_words)
# Term document matrix with all the previous transformation
tdm = TermDocumentMatrix(corpus)
```


## Explore the data

How many instances belong to what authors.
```{r}
ggplot(df, aes(x=author, color=author, fill=author)) +
  geom_bar()
```

```{r}
tdm = TermDocumentMatrix(corpus)
fr=rowSums(as.matrix(tdm))

high.fr=tail(sort(fr),n=20)
hfp.df=as.data.frame(sort(high.fr))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.fr), high.fr)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```
### Check common words of each author

Set the colours to be used in wordclouds 
```{r}
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:2)]

set.seed(1234)
```

#### EAP
Edgar Allen Poe
```{r}
# Make transformations
corpus.EAP = VCorpus(DataframeSource(df[df$author == "EAP",]))
corpus.EAP = tm_map(corpus.EAP, content_transformer(tolower))
corpus.EAP = tm_map(corpus.EAP, removeWords, c(stopwords(),new_stop_words))
corpus.EAP = tm_map(corpus.EAP, removePunctuation)
corpus.EAP = tm_map(corpus.EAP, removeNumbers)
corpus.EAP = tm_map(corpus.EAP, stripWhitespace)

# Term document Matrix
tdm_EAP = TermDocumentMatrix(corpus.EAP)
fr_EAP = rowSums(as.matrix(tdm_EAP))
```

Barplot most common words
```{r}

high.fr=tail(sort(fr_EAP),n=20)
hfp.df=as.data.frame(sort(high.fr))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.fr), high.fr)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")

```
Wordcloud
```{r}
word.cloud=wordcloud(words=names(fr_EAP), freq=fr_EAP, scale=c(2.5,.1),
                     min.freq = 100, random.order=F, color=pal)
```
Bigrams

```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

tdm_EAP.bigram = TermDocumentMatrix(corpus.EAP,
                                control = list (tokenize = BigramTokenizer))
```

Most common bigrams (barplot)
```{r}
freq = sort(rowSums(as.matrix(tdm_EAP.bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
```

Most common bigrams (wordcloud)
```{r}
wordcloud(freq.df$word,freq.df$freq,max.words=100,random.order = F, scale=c(3,.2), color=pal)
```

#### HPL
HP Lovecraft
```{r}
corpus.HPL = VCorpus(DataframeSource(df[df$author == "HPL",]))
corpus.HPL = tm_map(corpus.HPL, content_transformer(tolower))
corpus.HPL = tm_map(corpus.HPL, removeWords, c(stopwords(),new_stop_words))
corpus.HPL = tm_map(corpus.HPL, removePunctuation)
corpus.HPL = tm_map(corpus.HPL, removeNumbers)
corpus.HPL = tm_map(corpus.HPL, stripWhitespace)

tdm_HPL = TermDocumentMatrix(corpus.HPL)
fr_HPL = rowSums(as.matrix(tdm_HPL))
```

Barplot most common words
```{r}
high.fr=tail(sort(fr_HPL),n=20)
hfp.df=as.data.frame(sort(high.fr))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.fr), high.fr)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")

```
Wordcloud 
```{r}
word.cloud=wordcloud(words=names(fr_HPL), freq=fr_HPL, scale=c(3,.1),
                     min.freq = 100, random.order=F, color=pal)
```
Bigrams

```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

tdm_HPL.bigram = TermDocumentMatrix(corpus.HPL,
                                control = list (tokenize = BigramTokenizer))
```

Most common bigrams (barplot)
```{r}
freq = sort(rowSums(as.matrix(tdm_HPL.bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
```

Most common bigrams (wordcloud)
```{r}
wordcloud(freq.df$word,freq.df$freq,max.words=100,random.order = F, scale=c(2.5,.3), color=pal)
```
Old man and old woman are some of the most common bigrams, which seems interesting, in fact old man appears 53 times being the most common word by far.

#### MWS
```{r}

corpus.MWS = VCorpus(DataframeSource(df[df$author == "MWS",]))
corpus.MWS = tm_map(corpus.MWS, content_transformer(tolower))
corpus.MWS = tm_map(corpus.MWS, removeWords, c(stopwords(),new_stop_words))
corpus.MWS = tm_map(corpus.MWS, removePunctuation)
corpus.MWS = tm_map(corpus.MWS, removeNumbers)
corpus.MWS = tm_map(corpus.MWS, stripWhitespace)

tdm_MWS = TermDocumentMatrix(corpus.MWS)
fr_MWS = rowSums(as.matrix(tdm_MWS))
```

Barplot most common words
```{r}
high.fr=tail(sort(fr_MWS),n=20)
hfp.df=as.data.frame(sort(high.fr))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.fr), high.fr)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```
Wordcloud
```{r}
word.cloud=wordcloud(words=names(fr_MWS), freq=fr_MWS, scale=c(3,.1),
                     min.freq = 100, random.order=F, color=pal)
```
Bigrams

```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

tdm_MWS.bigram = TermDocumentMatrix(corpus.MWS,
                                control = list (tokenize = BigramTokenizer))
```

Most common bigrams (barplot)
```{r}
freq = sort(rowSums(as.matrix(tdm_MWS.bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
```

Most common bigrams (wordcloud)
```{r}
wordcloud(freq.df$word,freq.df$freq,max.words=100,random.order = F, scale=c(2.2,.3), color=pal)
```

Total words and distinct words (how many distinct words an author uses) of each author

```{r}
df_numWords <- data.frame(numWordsUnique=c(length(fr_EAP), length(fr_HPL), length(fr_MWS)),
                          numWords = c(sum(unname(fr_EAP)), sum(unname(fr_HPL)), sum(unname(fr_MWS))),
                          author=c("EAP", "HPL", "MWS"))
df_numWords <- melt(df_numWords, id.vars = "author")

ggplot(df_numWords, aes(x=author, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')

#ggplot(df_distincts, x=author, aes(y = numWords))+geom_bar()
```
## Topic modelling

Topic modeling is a technique for unsupervised classification of documents. This is similar to perform clustering on numerical data, finding natural groups of items with no previous knowledge of the data.  
  
In this project we are going to use Latent Dirichlet allocation (LDA). For creating LDA models we are going to use the following function.

Original function from:
https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling#Setting-up-our-workspace

Also in the documentation/tutorial of Tidy:
https://www.tidytextmining.com/topicmodeling.html#word-topic-probabilities


```{r}
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA

top_terms_by_topic_LDA <- function(corpus,  plot = T, number_of_topics = 4) {
  
  # For this task we need a document term matrix instead of a term document matrix
  dtm <- DocumentTermMatrix(corpus)
  ui = unique(dtm$i)
  dtm = dtm[ui,]
  
  # preform LDA & get the words/topic 
  lda_model <- LDA(dtm, k = number_of_topics, control = list(seed = 7))
  topics <- tidy(lda_model, matrix = "beta")
  
  # get the top ten terms for each topic
  top_terms <- topics  %>% # take the topics data frame and..
    group_by(topic) %>% # treat each topic as a different group
    top_n(10, beta) %>% # get the top 10 most informative words
    ungroup() %>% # ungroup
    arrange(topic, -beta) # arrange words in descending informativeness

  # if the user asks for a plot (TRUE by default)
  if(plot == T){
    # plot the top ten terms for each topic in order
    top_terms %>% # take the top terms
      mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
      ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
      geom_col(show.legend = FALSE) + # as a bar plot
      facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
      labs(x = NULL, y = "Beta") + # no x label, change y label 
      coord_flip() # turn bars sideways
  }else{ 
    # if the user does not request a plot
    # return de lda_model
    return (lda_model)
    # return a list of sorted terms instead
    #return(top_terms)
  }
}
```

we can try to get the top terms of each documents for 2 topics 
```{r}
top_terms_by_topic_LDA(corpus, plot = T, number_of_topics = 2)
```
we can try to get the top terms of each documents for 3 topics (hoping each topic belongs to 1 of the 3 authors)

```{r}
top_terms_by_topic_LDA(corpus, plot = T, number_of_topics = 3)
```
Topic 3 contains some of most used words by the author HPL (seem, old, night)

```{r}
lda_model <- top_terms_by_topic_LDA(corpus, plot = F, number_of_topics = 3)
chapters_gamma <- tidy(lda_model, matrix = "gamma")

df_topics <- merge(df, chapters_gamma, by.x="doc_id", by.y="document")
df_topics$doc_id <- NULL
df_topics$text <- NULL
df_topics$gamma <- NULL

```

Authors distribution in topic 1
```{r}
df_topics_T1 = df_topics[df_topics$topic == 1,]
ggplot(df_topics_T1, aes(x=author, fill=author)) +
    geom_bar()
```
Authors distribution in topic 2

```{r}
df_topics_T2 = df_topics[df_topics$topic == 2,]
ggplot(df_topics_T2, aes(x=author, fill=author)) +
    geom_bar()
```
  
```{r}
df_topics_T3 = df_topics[df_topics$topic == 3,]
ggplot(df_topics_T3, aes(x=author, fill=author)) +
    geom_bar()
```

Also we can try to run LDA with for each of the authors

For EAP
```{r}
top_terms_by_topic_LDA(corpus.EAP, plot = T, number_of_topics = 2)
```
HPL
```{r}
top_terms_by_topic_LDA(corpus.HPL, plot = T, number_of_topics = 2)
```

MWS
```{r}
top_terms_by_topic_LDA(corpus.MWS, plot = T, number_of_topics = 2)
```
For MWS, topic 1 could mean names or character since it contains raymond and adrian terms. Topic 2 contains terms love, heart, eyes, life (positive/romantic words maybe)  
  
As you can see the results are not very interpretable, also i do not know this authors and their work.

One possible solution could be using a document term matrix containing only nouns or adjective + nouns.


